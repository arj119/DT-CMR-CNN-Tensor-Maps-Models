# -*- coding: utf-8 -*-
"""tf2train_MedGANish.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lbVWBMFLtjEXsfp4Hmaybh_5rc3gAeq6

# Investigating Pix2pix To Produce DT Data From Undersampled Scans

## Mount drive folders containing dataset
"""

# Commented out IPython magic to ensure Python compatibility.
# from google.colab import drive
# drive.mount('/content/drive')
# %ls

from tensorflow.python.keras import optimizers
from tensorflow.python.keras import models
from tensorflow.python.keras import losses
from tensorflow.python.keras import layers
import tensorflow as tf
from tensorflow.python.keras import backend as K
from sklearn.model_selection import train_test_split
import pickle
import numpy as np
import pandas as pd
from datetime import datetime, date
import functools
import sys
import glob
import os
import time
from IPython import get_ipython
from matplotlib import pyplot as plt
from IPython import display

# Commented out IPython magic to ensure Python compatibility.
# pwd
path = os.path.join("drive", "My Drive", "Colab Notebooks")
os.chdir(path)
# %ls

"""## Hyperparameters"""

# DTI parameter to train (fa, ha, e2a, dt etc...)
dti_param = 'dt'
# the code is the input number of averages, lets start with 4
dataset_code = '4' 

# set up image size and training parameters
IMG_WIDTH = 128
IMG_HEIGHT = 128
INPUT_CHANNELS = 13
OUTPUT_CHANNELS = 6
BUFFER_SIZE = 225
BATCH_SIZE = 1
EPOCHS = 200
SAVE_CYCLE = 50
SCALE_TARGET = 100

print('====================================================')
print('DTI parameter: ' + dti_param)
print('Dataset code: ' + dataset_code)

# cnn filename with date dti parameter and dataset code
date_time_str = "{:%Y_%m_%d_%H_%M_%S}".format(datetime.now())
# cnn_name_new = dti_param + '_' + dataset_code + \
#     '_u_net_weights_' + date_time_str + '.hdf5'
# current_dir = sys.path[0]

# fix the random seed
np.random.seed(1)

input_dir = 'input_data_' + dataset_code
output_dir = 'output_data_' + dti_param

# list of arrays
input_list = glob.glob(os.path.join(input_dir, '*.npz'))
input_list.sort()

list_outputs = glob.glob(os.path.join(output_dir, '*.npz'))
list_outputs.sort()

# split list of inputs and outputs in train, validate and test
train_inputs, val_inputs, train_outputs, val_outputs = \
    train_test_split(input_list, list_outputs,
                     test_size=0.2, random_state=42)

val_inputs, test_inputs, val_outputs, test_outputs = \
    train_test_split(val_inputs, val_outputs,
                     test_size=0.5, random_state=42)

num_train_examples = len(train_inputs)
num_val_examples = len(val_inputs)
num_test_examples = len(test_inputs)

print("Number of training examples: {}".format(num_train_examples))
print("Number of validation examples: {}".format(num_val_examples))
print("Number of test examples: {}".format(num_test_examples))
print('====================================================')

# save the test subject list in a csv file
d = {'Input': test_inputs, 'Output': test_outputs}
df = pd.DataFrame(d)
df.to_csv('test_subject_list_' + dti_param + '.csv', index=False)

"""## Utility Functions"""

#Normalise array to range [-1,1]
def normalise_matrices(array):
    array = np.true_divide(array, 127.5) - 1
    return array

def scale_output(array):
    array = array * SCALE_TARGET
    return array


def load_data_into_array(name, input=True, normalise_input=True):
  array = np.load(name)
  array = array['arr_0']
  array = np.float32(array)
  if input:
    if normalise_input:
      array = normalise_matrices(array)
  else:
    array = scale_output(array)
  array = np.expand_dims(array, axis=0)
  return array

def data_info(data_arr, isOutput=False):
  data = data_arr.flatten()
  out = ""
  if isOutput:
    print("Output data from", dti_param)
  else:
    print("Input data from", dti_param)
  print('min', np.amin(data))
  print('max', np.amax(data))
  print('mean', np.mean(data))
  print()

#  Data augmentation functions
def random_crop(img_in, img_out):
    #  stack images before rotation so that the rotation is the same
    stacked_image = tf.concat([img_in, img_out], axis=2)
    cropped_image = tf.image.random_crop(
        stacked_image, size=[IMG_HEIGHT, IMG_WIDTH, INPUT_CHANNELS + OUTPUT_CHANNELS])

    r_img_in = cropped_image[:, :, 0:INPUT_CHANNELS]
    r_img_out = cropped_image[:, :, INPUT_CHANNELS:]
    # if OUTPUT_CHANNELS == 1:
    #     r_img_out = tf.expand_dims(r_img_out, axis=2)

    return r_img_in, r_img_out


def augment(img_in, img_out):
    # Add 20 pixels padding
    add_n = 20
    img_in = tf.image.resize_with_crop_or_pad(
        img_in, IMG_HEIGHT+add_n, IMG_WIDTH+add_n)
    img_out = tf.image.resize_with_crop_or_pad(
        img_out, IMG_HEIGHT+add_n, IMG_WIDTH+add_n)
    img_in, img_out = random_crop(img_in, img_out)

    return img_in, img_out

"""## Data Preprocessing"""

# load numpy arrays
def load_npz_files(input_list, output_list, dti_param):
    ''' load input and output arrays from lists of npz files '''
    # loop through the list and stack arrays
    for idx, val in enumerate(input_list):
        if idx == 0:
            input_arrays = load_data_into_array(val, normalise_input=True)
        else:
            temp = load_data_into_array(val, normalise_input=True)
            input_arrays = np.concatenate((input_arrays, temp), axis=0)

    for idx, val in enumerate(output_list):
        if idx == 0:
            output_arrays = load_data_into_array(val, input=False)
        else:
            temp = load_data_into_array(val, input=False)
            output_arrays = np.concatenate((output_arrays, temp), axis=0)

    if OUTPUT_CHANNELS == 1:
        output_arrays = np.expand_dims(output_arrays, axis=-1)

    # check there is no nan or inf
    input_arrays = np.nan_to_num(input_arrays)
    output_arrays = np.nan_to_num(output_arrays)
    assert np.isnan(np.sum(input_arrays)) == False, 'input_array has nans'
    assert np.isnan(np.sum(output_arrays)) == False, 'output_arrays has nans'
    assert np.isinf(np.sum(input_arrays)) == False, 'input_array has infs'
    assert np.isinf(np.sum(output_arrays)) == False, 'output_arrays has infs'

    assert input_arrays.shape[0] == output_arrays.shape[0]
    return input_arrays, output_arrays

def data_info(data_arr, dti_param,isOutput):
  data = data_arr.flatten()
  if(isOutput):
    print("Output data from", dti_param)
  else:
    print("Input data from", dti_param)
  print('min', np.amin(data))
  print('max', np.amax(data))
  print('mean', np.mean(data))
  print()

# get the tensorflow train and validate datasets to tensorflow
def get_baseline_dataset(input_list, output_list, dti_param):

    # read arrays
    input_arrays, output_arrays = load_npz_files(
        input_list, output_list, dti_param)


    dataset = tf.data.Dataset.from_tensor_slices((input_arrays, output_arrays))
    data_info(input_arrays, dti_param, False)
    data_info(output_arrays, dti_param, True)

    return dataset


# Training Dataset
print("Training Dataset")
train_ds = get_baseline_dataset(train_inputs, train_outputs, dti_param)
train_ds = train_ds.map(augment,
                        num_parallel_calls=tf.data.experimental.AUTOTUNE)
train_ds = train_ds.shuffle(BUFFER_SIZE)
train_ds = train_ds.batch(BATCH_SIZE)
train_ds = train_ds.cache()

# Validation Dataset
print("Validation Dataset")
val_ds = get_baseline_dataset(val_inputs, val_outputs, dti_param)
val_ds = val_ds.batch(BATCH_SIZE)

# Training Dataset
print("Test Dataset")
test_ds = get_baseline_dataset(test_inputs, test_outputs, dti_param)
test_ds = test_ds.batch(BATCH_SIZE)

"""## Generator

* The architecture of generator is a CasNet generator consisting of concatenated modified U-Net.
* Each block in the encoder is (Conv -> Batchnorm -> Leaky ReLU)
* Each block in the decoder is (Transposed Conv -> Batchnorm -> Dropout(applied to the first 3 blocks) -> ReLU)
* There are skip connections between the encoder and decoder(as in U-Net).
"""

# %%
def downsample(filters, size, apply_batchnorm=True):
    initializer = tf.random_normal_initializer(0., 0.02)

    result = tf.keras.Sequential()
    result.add(
        tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',
                               kernel_initializer=initializer, use_bias=False))
        
    if apply_batchnorm:
        result.add(tf.keras.layers.BatchNormalization())

    result.add(tf.keras.layers.LeakyReLU())

    return result


# %%
temp_array = np.load(input_list[0])
temp_array = temp_array['arr_0']
temp_array = tf.convert_to_tensor(temp_array, dtype=tf.float32)
down_model = downsample(INPUT_CHANNELS, 4)
down_result = down_model(tf.expand_dims(temp_array, 0))
print(down_result.shape)


# %%
def upsample(filters, size, apply_dropout=False):
    initializer = tf.random_normal_initializer(0., 0.02)

    result = tf.keras.Sequential()
    result.add(
        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,
                                        padding='same',
                                        kernel_initializer=initializer,
                                        use_bias=False))

    result.add(tf.keras.layers.BatchNormalization())

    if apply_dropout:
        result.add(tf.keras.layers.Dropout(0.5))

    result.add(tf.keras.layers.ReLU())

    return result


# %%
up_model = upsample(13, 4)
up_result = up_model(down_result)
print(up_result.shape)


# %%
def Generator(u_blocks):
    # Input image shape (bs, 128, 128, INPUT_CHANNELS)
    inputs = tf.keras.layers.Input(
        shape=[IMG_HEIGHT, IMG_WIDTH, INPUT_CHANNELS])
    
    x = inputs

    for i in range(u_blocks):
      down_stack = [
          downsample(64, 4, apply_batchnorm=False),  # (bs, 64, 64, 64)
          downsample(128, 4),  # (bs, 32, 32, 128)
          downsample(256, 4),  # (bs, 16, 16, 256)
          downsample(512, 4),  # (bs, 8, 8, 512)
          downsample(512, 4),  # (bs, 4, 4, 512)
          downsample(512, 4),  # (bs, 2, 2, 512)
          downsample(512, 4),  # (bs, 1, 1, 512)
      ]

      up_stack = [
          upsample(512, 4, apply_dropout=True),  # (bs, 2, 2, 1024)
          upsample(512, 4, apply_dropout=True),  # (bs, 4, 4, 1024)
          upsample(512, 4, apply_dropout=True),  # (bs, 8, 8, 1024)
          upsample(512, 4),  # (bs, 16, 16, 1024)
          upsample(256, 4),  # (bs, 32, 32, 512)
          upsample(128, 4),  # (bs, 64, 64, 256)
          upsample(64, 4),  # (bs, 128, 128, 128)
      ]


      initializer = tf.random_normal_initializer(0., 0.02)
      last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,
                                            strides=2,
                                            padding='same',
                                            kernel_initializer=initializer,
                                            activation= 'tanh')

      # Downsampling through the model
      skips = []
      for down in down_stack:
          x = down(x)
          skips.append(x)

      skips = reversed(skips[:-1])

      # Upsampling and establishing the skip connections
      for up, skip in zip(up_stack, skips):
          x = up(x)
          x = tf.keras.layers.Concatenate()([x, skip])



      x = last(x)

    
    return tf.keras.Model(inputs=inputs, outputs=x)


# %% Generator is n U-Blocks
generator = Generator(1)
tf.keras.utils.plot_model(generator)

# %%
gen_output = generator(temp_array[tf.newaxis, ...], training=False)
plt.imshow(gen_output[0, :, :, 0])
plt.show()

"""### Generator Loss
* It is a sigmoid cross entropy loss of the generated images and an **array of ones**.
* The [paper](https://arxiv.org/abs/1611.07004) also includes L1 loss which is MAE (mean absolute error) between the generated image and the target image.
* This allows the generated image to become structurally similar to the target image.
* The formula to calculate the total generator loss = gan_loss + LAMBDA * l1_loss, where LAMBDA = 100. This value was decided by the authors of the [paper](https://arxiv.org/abs/1611.07004).
"""

# %%
L1_LOSS_LAMBDA = 100


def generator_loss(disc_generated_output, gen_output, target):
    gan_loss = loss_object(tf.ones_like(
        disc_generated_output), disc_generated_output)

    # mean absolute error
    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))

    total_gen_loss = gan_loss + (L1_LOSS_LAMBDA * l1_loss)

    return total_gen_loss, gan_loss, l1_loss

"""## Discriminator
* The Discriminator is a PatchGAN.
* Each block in the discriminator is (Conv -> BatchNorm -> Leaky ReLU)
* The shape of the output after the last layer is (batch_size, 30, 30, 1)
* Each 30x30 patch of the output classifies a 70x70 portion of the input image (such an architecture is called a PatchGAN).
* Discriminator receives 2 inputs.
* Input image and the target image, which it should classify as real.
* Input image and the generated image (output of generator), which it should classify as fake.
* We concatenate these 2 inputs together in the code (`tf.concat([inp, tar], axis=-1)`)

### Build the Discriminator
"""

def Discriminator():
    initializer = tf.random_normal_initializer(0., 0.02)

    inp = tf.keras.layers.Input(
        shape=[IMG_HEIGHT, IMG_WIDTH, INPUT_CHANNELS], name='input_image')
    tar = tf.keras.layers.Input(
        shape=[IMG_HEIGHT, IMG_WIDTH, OUTPUT_CHANNELS], name='target_image')

    x = tf.keras.layers.concatenate([inp, tar])  # (bs, 256, 256, channels*2)

    down1 = downsample(64, 4, False)(x)  # (bs, 128, 128, 64)
    down2 = downsample(128, 4)(down1)  # (bs, 64, 64, 128)
    down3 = downsample(256, 4)(down2)  # (bs, 32, 32, 256)

    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (bs, 34, 34, 256)
    conv = tf.keras.layers.Conv2D(512, 4, strides=1,
                                  kernel_initializer=initializer,
                                  use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)

    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)

    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)

    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)

    last = tf.keras.layers.Conv2D(1, 4, strides=1,
                                  kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 30, 1)

    return tf.keras.Model(inputs=[inp, tar], outputs=last)


# %%
discriminator = Discriminator()
tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=128)

# %%
disc_out = discriminator(
    [temp_array[tf.newaxis, ...], gen_output], training=False)
plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')
plt.colorbar()
plt.show()

"""### Discriminator loss
* The discriminator loss function takes 2 inputs; **real images, generated images**
* real_loss is a sigmoid cross entropy loss of the **real images** and an **array of ones(since these are the real images)**
* generated_loss is a sigmoid cross entropy loss of the **generated images** and an **array of zeros(since these are the fake images)**
* Then the total_loss is the sum of real_loss and the generated_loss

"""

# %%
loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)

# %%
def discriminator_loss(disc_real_output, disc_generated_output):
    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)

    generated_loss = loss_object(tf.zeros_like(
        disc_generated_output), disc_generated_output)

    total_disc_loss = real_loss + generated_loss

    return total_disc_loss

"""## VGG19 Set Up For Style And Transfer Losses

### Load VGG19 pretrained on ImageNet
"""

vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')

print()
for layer in vgg.layers:
  print(layer.name)

"""### Choose Style and Content Layers"""

content_layers = ['block5_conv2'] 

style_layers = ['block1_conv1',
                'block2_conv1',
                'block3_conv1', 
                'block4_conv1', 
                'block5_conv1']

num_content_layers = len(content_layers)
num_style_layers = len(style_layers)

"""### Build Style Content Feature Extractor"""

# Save ndarray of shape (128,128,1) to 128x128 rgb 3 channel image through plot
def imshow(image, fname='tmp.png'):
  if len(image.shape) > 3:
    image = tf.squeeze(image, axis=0)

  img_path = os.path.join('tmp', fname)
  fig = plt.figure(frameon=False)
  fig.set_size_inches(4,4)
  ax = plt.Axes(fig, [0., 0., 1., 1.])
  ax.set_axis_off()
  fig.add_axes(ax)
  ax.imshow(image, aspect='auto')
  fig.savefig(img_path, transparent=True, pad_inches=0, dpi=128)
  return img_path

# Load Image from path as (512,512,3)
def load_img(path_to_img):
  max_dim = 512
  img = tf.io.read_file(path_to_img)
  img = tf.image.decode_image(img, channels=3)
  img = tf.image.convert_image_dtype(img, tf.float32)

  shape = tf.cast(tf.shape(img)[:-1], tf.float32)
  long_dim = max(shape)
  scale = max_dim / long_dim

  new_shape = tf.cast(shape * scale, tf.int32)

  img = tf.image.resize(img, new_shape)
  img = img[tf.newaxis, :]
  return img

def vgg_layers(layer_names):
  """ Creates a vgg model that returns a list of intermediate output values."""
  # Load our model. Load pretrained VGG, trained on imagenet data
  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')
  vgg.trainable = False
  
  outputs = [vgg.get_layer(name).output for name in layer_names]

  model = tf.keras.Model([vgg.input], outputs)
  return model

(example_input, example_target) = list(val_ds.as_numpy_iterator())[0]
# path = imshow(example_target[0,:,:,5])
# image = load_img(path)
fig = plt.figure(frameon=False)
fig.set_size_inches(4,4)
ax = plt.Axes(fig, [0., 0., 1., 1.])
ax.set_axis_off()
fig.add_axes(ax)
ax.imshow(example_target[0,:,:,3:6] * 255, aspect='auto')
style_extractor = vgg_layers(style_layers)
style_outputs = style_extractor(example_target[:,:,:,3:6])
# print(np.shape(example_target[0,:,:,3:6]))
# Look at the statistics of each layer's output

for name, output in zip(style_layers, style_outputs):
  print(name)
  print("  shape: ", output.numpy().shape)
  print("  min: ", output.numpy().min())
  print("  max: ", output.numpy().max())
  print("  mean: ", output.numpy().mean())
  print()

"""#### Gram Matrix

The style of an image can be described by the means and correlations across the different feature maps. Calculate a Gram matrix that includes this information by taking the outer product of the feature vector with itself at each location, and averaging that outer product over all locations. This Gram matrix can be calculated for a particular layer as shown here:

https://www.tensorflow.org/tutorials/generative/style_transfer#extract_style_and_content


"""

def gram_matrix(input_tensor):
  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)
  input_shape = tf.shape(input_tensor)
  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)
  return result/(num_locations)

"""#### Style Content Extractor Class"""

class StyleContentModel(tf.keras.models.Model):
  def __init__(self, style_layers, content_layers):
    super(StyleContentModel, self).__init__()
    self.vgg =  vgg_layers(style_layers + content_layers)
    self.style_layers = style_layers
    self.content_layers = content_layers
    self.num_style_layers = len(style_layers)
    self.vgg.trainable = False

  def call(self, inputs):
    "Expects float input in [0,1]"
    inputs = inputs*255.0
    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)
    outputs = self.vgg(preprocessed_input)
    style_outputs, content_outputs = (outputs[:self.num_style_layers], 
                                      outputs[self.num_style_layers:])

    style_outputs = [gram_matrix(style_output)
                     for style_output in style_outputs]

    content_dict = {content_name:value 
                    for content_name, value 
                    in zip(self.content_layers, content_outputs)}

    style_dict = {style_name:value
                  for style_name, value
                  in zip(self.style_layers, style_outputs)}
    
    return {'content':content_dict, 'style':style_dict}

"""#### Define Style And Content Extractor


"""

extractor = StyleContentModel(style_layers, content_layers)

"""#### Example"""

# Example
(example_input, example_target) = list(val_ds.as_numpy_iterator())[0]
# path = imshow(example_target[0,:,:,5])
# image = load_img(path)

print(example_target[:,:,:,3:6].max());
print(example_target[:,:,:,3:6].min());
results = extractor(tf.constant(example_target[:,:,:,3:6] + example_target[:,:,:,3:6].min()))

print('Styles:')
for name, output in sorted(results['style'].items()):
  print("  ", name)
  print("    shape: ", output.numpy().shape)
  print("    min: ", output.numpy().min())
  print("    max: ", output.numpy().max())
  print("    mean: ", output.numpy().mean())
  print()

print("Contents:")
for name, output in sorted(results['content'].items()):
  print("  ", name)
  print("    shape: ", output.numpy().shape)
  print("    min: ", output.numpy().min())
  print("    max: ", output.numpy().max())
  print("    mean: ", output.numpy().mean())

"""#### Define Style And Content Loss

Take target and predicted image, run through feature extractor to get style and content values. After this calculate the style and content loss between the two images
"""

style_weight=1e-10
content_weight=1e-2

@tf.function
def style_content_loss(prediction, target):
    # if type(prediction) is tf.python.framework.ops.Tensor:
    #   prediction = tf.make_tensor_proto(prediction)
    #   prediction = tf.make_ndarray(prediction)
    # if type(target) is tf.python.framework.ops.Tensor:
    #   target = tf.make_tensor_proto(target)
    #   target = tf.make_ndarray(target)

    # Generate images from numpy arrays to pass as input into VGG19
    # prediction_fname = imshow(prediction[0,:,:,5], 'prediction.png')
    # target_fname = imshow(target[0,:,:,5], 'target.png')

    # # Load images
    # prediction_image = load_img(prediction_fname)
    # target_image = load_img(target_fname)

    # Load images
    prediction_image = prediction[:,:,:,3:6]
    target_image = target[:,:,:,3:6]

    # Feature extraction
    prediction_features = extractor(prediction_image)
    target_features = extractor(target_image)

    prediction_style = prediction_features['style']
    prediction_content = prediction_features['content']
    
    target_style = target_features['style']
    target_content = target_features['content']


    style_loss = tf.add_n([tf.reduce_mean((prediction_style[name]-target_style[name])**2) 
                           for name in prediction_style.keys()])
    style_loss *= style_weight / num_style_layers

    content_loss = tf.add_n([tf.reduce_mean((prediction_content[name]-target_content[name])**2) 
                             for name in prediction_content.keys()])
    content_loss *= content_weight / num_content_layers
    loss = style_loss + content_loss
    return loss

# Example
(example_input, example_target) = list(val_ds.as_numpy_iterator())[0]
example_prediction = generator(example_input, training=False)

print(style_content_loss(example_prediction, example_target).numpy())

# assert style_content_loss(example_target, example_target).numpy() == 0.0

"""## Define the Optimizers and Checkpoint-saver"""

# %%
generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)


# %%
checkpoint_dir = os.path.join('checkpoints', 'training_checkpoint_MedGan2_' + dataset_code)
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                 discriminator_optimizer=discriminator_optimizer,
                                 generator=generator,
                                 discriminator=discriminator)

manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=2)

"""## Generate Images

Write a function to plot some images during training.

* We pass images from the test dataset to the generator.
* The generator will then translate the input image into the output.
* Last step is to plot the predictions and **voila!**

Note: The `training=True` is intentional here since
we want the batch statistics while running the model
on the test dataset. If we use training=False, we will get
the accumulated statistics learned from the training dataset (which we don't want)
"""

def denormalise_output(arr) :
    return arr / SCALE_TARGET


def generate_images(model, test_input, tar):
    prediction = model(test_input, training=True)
    plt.figure(figsize=(30, 30))
    target = denormalise_output(tar[0, :, :, 5])
    prediction = denormalise_output(prediction[0, :, :, 5])
    absDiff = abs(target - prediction)
    meanAbsDiff = np.mean(absDiff)
    medianAbsDiff = np.median(absDiff)
    meanRelativeDifference = abs(np.mean(target) - np.mean(prediction))/np.mean(target) * 100
    display_list = [test_input[0, :, :, 5], target, prediction]

    title = ["Input",'Ground Truth', 'Predicted Image']

    for i in range(3):
        plt.subplot(1, 3, i+1)
        plt.title(title[i])
        im = plt.imshow(display_list[i])
        # plt.clim(0, 0.003)
        plt.colorbar(im, fraction=0.046, pad=0.04)
        plt.axis('off')
    plt.show()
    print("Mean absolute error: " + str(meanAbsDiff))
    print("Median absolute error: " + str(medianAbsDiff))
    print("Mean difference percentage error: " + str(meanRelativeDifference))



# %%
for (example_input, example_target) in val_ds.take(1):
    generate_images(generator, example_input, example_target)

"""## Training
* For each example input generate an output.
* The discriminator receives the input_image and the generated image as the first input. The second input is the input_image and the target_image.
* Next, we calculate the generator and the discriminator loss.
* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.
* Then log the losses to TensorBoard.
"""

# %%
log_dir = os.path.join("logs", "MedGANish", dataset_code)

summary_writer = tf.summary.create_file_writer(
    os.path.join(log_dir, "fit", datetime.now().strftime("%Y%m%d-%H%M%S")))


# %%
@tf.function
def train_step(input_image, target, epoch):
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        gen_output = generator(input_image, training=True)

        disc_real_output = discriminator([input_image, target], training=True)
        disc_generated_output = discriminator(
            [input_image, gen_output], training=True)

        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(
            disc_generated_output, gen_output, target)
        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)

        style_and_content_loss = style_content_loss(gen_output, target)

        gen_total_loss = gen_total_loss + style_and_content_loss

    generator_gradients = gen_tape.gradient(gen_total_loss,
                                            generator.trainable_variables)
    discriminator_gradients = disc_tape.gradient(disc_loss,
                                                 discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(generator_gradients,
                                            generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,
                                                discriminator.trainable_variables))

    with summary_writer.as_default():
        tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)
        tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)
        tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)
        tf.summary.scalar('disc_loss', disc_loss, step=epoch)
        tf.summary.scalar('style_and_content_loss', style_and_content_loss, step=epoch)

"""### The actual training loop:
* Iterates over the number of epochs.
* On each epoch it clears the display, and runs `generate_images` to show it's progress.
* On each epoch it iterates over the training dataset, printing a '.' for each example.
* It saves a checkpoint every 20 epochs.
"""

def fit(train_ds, epochs, test_ds):

    checkpoint.restore(manager.latest_checkpoint)
    if manager.latest_checkpoint:
        print("Restored from {}".format(manager.latest_checkpoint))
    else:
        print("Initializing from scratch.")


    for epoch in range(epochs):
        start = time.time()

        display.clear_output(wait=True)
        
        for (example_input, example_target) in val_ds.take(1):
            generate_images(generator, example_input, example_target)
        print("Epoch: ", epoch)

        # Train
        for n, (input_image, target) in train_ds.enumerate():
            print('.', end='')
            if (n+1) % 100 == 0:
                print()
            train_step(input_image, target, epoch)
        print()

        # saving (checkpoint) the model every EPOCHS epochs
        if (epoch + 1) % SAVE_CYCLE == 0:
            save_path = manager.save()
            print("Saved checkpoint for epoch {}: {}".format(
                int(epoch), save_path))

        print('Time taken for epoch {} is {} sec\n'.format(epoch + 1,
                                                           time.time()-start))
    # checkpoint.save(file_prefix=checkpoint_prefix)

"""### Tensorboard viewer"""

# #docs_infra: no_execute
get_ipython().run_line_magic('load_ext', 'tensorboard')
get_ipython().run_line_magic('tensorboard', '--logdir {log_dir}')

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard

"""### Now run the training loop:

"""

fit(train_ds, 50 + 1, val_ds)

"""## Results"""

#Load model
checkpoint.restore(manager.latest_checkpoint)
if manager.latest_checkpoint:
    print("Restored from {}".format(manager.latest_checkpoint))
else:
    print("Initializing from scratch.")

#Save Generator
generator.save("./Arjun_generator_weights/CasNetGen/model.hdf5", save_format='h5')

for (example_input, example_target) in test_ds:
            generate_images(generator, example_input, example_target)

"""## Clean up"""

# Commented out IPython magic to ensure Python compatibility.
# Clean up temporary images
# %rm -rf tmp